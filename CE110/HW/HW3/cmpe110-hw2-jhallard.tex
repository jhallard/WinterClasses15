\documentclass[a4paper,11pt]{article}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{url}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdflscape}
\usepackage{afterpage}
\usepackage[margin=0.6in]{geometry}

% \setlength{\voffset}{-0.5in}
\setlength{\headsep}{7pt}
\newcommand{\suchthat}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}
\newcommand{\answer}{\textbf{Answer : }}
\newcommand{\T}{\texttt}
\newcommand{\V}{\verb}
\newcommand{\benu}{\begin{enumerate}}
\newcommand{\enu}{\end{enumerate}}


%===========---------================
% Author John H Allard
% CMPE 110, HW #2
% December 10th, 2014
%===========---------================

\begin{document}
    % \vspace*{\stretch{-0.5}}
   \begin{center}
      \Large\textbf{CMPE 110 Homework \#3}\\
      \large\texttt{John Allard} \\
      \small\texttt{February 23nd, 2015} \\
      \small\texttt{jhallard@ucsc.edu}
   \end{center}
   % \vspace*{\stretch{-0.5}}


\benu

%==============================%
% ======== PROBLEM #1 ======== %
%==============================%
\item \textbf{Question \#1 - Basic Cache} \\

  Consider a 512-KByte cache with 32 word cache lines.This cache uses write-back scheme, and the address is 32 bits wide. (The three tables for parts A, B, and C have been condensed into a single table found below part C).

  Note - Since it is not stated, I am assuming that the lower 2 bits are hard-coded as \texttt{00}, which means all accesses are word-aligned. I am also including both the word offset and byte offset in my calculation of cacheline offset, as we are supposed to do as stated in question \@436 on Piazza.

\benu
% ======= Question 1.A ======= %
\item \textbf{Question 1.A Direct-Mapped, Cache fields}
Assume the cache is direct mapped. Fill in the table below to specify the size of each address field.

\textbf{Explanation} - For a direct-mapped cache, we need to have a 5-bit word offset because each cache-line is 32 words wide, we then need the 2 hard-coded bits to account for byte offset within each word, for a total of 7 bits for cacheline offset. Since our cache is 52-KByte large, and each line is 32 words,  we have 4,096 cache lines. The amount of bits needed to address this amount of cache-lines is 12 ($2^{12} = 4,096$). This leaves 13 bits for the tag.

% ======== Question 1.B ======= %
\item \textbf{Question 1.B Fully Associative Cache Fields} \\
Assume the cache is fully associative,  fill in the table below to specify the size of each address field. \\


\textbf{Explanation} - For a fully-associative cache, we still need 7 bits for the offset, which determines which of the 32 words in a cache-line our data is stored at. Since fully-associative caches do not use an index, this leaves 25 bits left for the tag. 

% ======= Question 1.C ========= %

\item \textbf{Question 1.C 8-Way Set-Associative, Cache Fields} \\
Assume the cache is 8-way associative, fill in the table below to specify the size of each address field. \\

\textbf{Explanation} - Once again, we need 7 bits for the offset because each cache-line is 32-words wide. With 512Kbyte of memory and an 8-way set-associative cache, we will need 11 bits to state which set we are looking for ($2^{9} =4,096/8$). Finally we have 16 bits left to form the tag.

\begin{table}[H]
\caption{Q 1A, 1B, and 1C Calculations} \label{tab:q1calc} 
\begin{center}
\begin{tabular}{| l | l | l | l |} \hline 
   Field              &  Size (1A) & Size (1B) & Size (1C)     \\ \hline
   Cache line offset  &  7         &   7       &   7           \\ \hline
   Cache line index   &  12        &   0       &   9           \\ \hline
   Tag                &  13        &   25      &   16          \\ \hline
\end{tabular}
\end{center}
\end{table}



% ======= Question 1.D ========= %
\item \textbf{Question 1.D Direct Mapped Cache Transactions } \\

Assume the cache is direct-mapped. Fill in the table on the next page to identify the content of the cache after each of the following memory accesses. Assume the cache is initially empty (aka cold). Specify if an entry causes another line to be replaced from the cache, and if an entry has to write its data back to memory. For the data column, specify the data in the block by referring to its address like M[address]


\begin{table}[H]
\caption{Q1D Calculations} \label{tab:q1calc} 
\begin{center}
\begin{tabular}{| l | l | l | l | l | l | l | l | l |}
\hline
Address       & Request    & Cacheline Ind & Valid  & Modified  & Tag    & Data            & Caused Replace  & Write-back?    \\ \hline
\T{0x128}     &  \T{read}  & \T{ 2 }       & \T{1}  & \T{0}     & \T{0}  & \T{M[0x100]}    &      \T{0}      & \T{0}    \\ \hline
\T{0xF40}     &  \T{write} & \T{30 }       & \T{1}  & \T{1}     & \T{0}  & \T{D[0xF00]}    &      \T{0}      & \T{0}    \\ \hline
\T{0xC00024}  &  \T{read}  & \T{ 0 }       & \T{1}  & \T{0}     & \T{24} & \T{M[0xC00000]} &      \T{0}      & \T{0}    \\ \hline
\T{0x014}     &  \T{write} & \T{ 0 }       & \T{1}  & \T{1}     & \T{0}  & \T{D[0x000]}    &      \T{1}      & \T{0}    \\ \hline
\T{0x100F44}  &  \T{read}  & \T{ 30}       & \T{1}  & \T{1}     & \T{2}  & \T{D[0x100F00]} &      \T{1}      & \T{1}    \\ \hline
\end{tabular}
\end{center}
\end{table}

\textbf{Explanation} - 


% ======= Question 1.E ========= %
\item \textbf{Question 1.E 8-Way Set Associative, Cache Transactions } \\
Assume the cache is 8-way set-associative. Fill the table below to identify the content of the cache after each of the following memory accesses. Assume the cache is empty in the beginning (also known as cold cache). Specify if an entry causes another line to be replaced from the cache, and if an entry has to write its data back to memory. For the data column, specify the data in the block by referring to its address like M[address]. Write accesses will modify the data, so lets indicate the data after a write access with D[address].

\begin{table}[H]
\caption{Q1E Calculations} \label{tab:q1calc2} 
\begin{center}
\begin{tabular}{| l | l | l | l | l | l | l | l | l |}
\hline
Address       & Request    & Cacheline Ind & Valid  & Modified  & Tag    & Data            & Caused Replace  & Write-back?    \\ \hline
\T{0x128}     &  \T{read}  & \T{ 2 }       & \T{0}  & \T{0}     & \T{0}  & \T{M[0x100]}    &      \T{0}      & \T{0}    \\ \hline
\T{0xF40}     &  \T{write} & \T{30 }       & \T{1}  & \T{1}     & \T{0}  & \T{D[0xF00]}    &      \T{0}      & \T{0}    \\ \hline
\T{0xC00024}  &  \T{read}  & \T{ 0 }       & \T{1}  & \T{0}     & \T{96} & \T{M[0xC00000]} &      \T{0}      & \T{0}    \\ \hline
\T{0x014}     &  \T{write} & \T{ 0 }       & \T{1}  & \T{1}     & \T{0}  & \T{D[0x000]}    &      \T{0}      & \T{0}    \\ \hline
\T{0x100F44}  &  \T{read}  & \T{ 30}       & \T{1}  & \T{0}     & \T{16} & \T{D[0x100F00]} &      \T{0}      & \T{0}    \\ \hline
\end{tabular}
\end{center}
\end{table}

\textbf{Explanation} -


\item \textbf{Question 1.F Overhead} \\
What is the overhead and actual size of the direct-mapped cache? What is the overhead and actual size of the 8-way set-associative cache? Does the structure change the overhead in terms of number of memory bits?

\answer 


\enu


%==============================%
% ======== PROBLEM #2 ======== %
%==============================%

\item \textbf{Question \#2 -  Impact of Cache Access Time} \\ 
In this problem, we will be comparing various microarchitectures for a simple data cache. For all parts, the memory requests use a 32-bit address, although you should assume that all addresses are word aligned. Since words are four bytes, this means the bottom two bits of the address will always be zero. All caches contain exactly eight cache lines, and each cache line contains four words (i.e., each cache line is 16 bytes long). Thus the total cache capacity is $8 \times 16B = 128B$ \par

This problem will require you to identify the critical path of a specific microarchitecture. The table below lists simplified delay equations for the cache hardware components. These delay equations are parameterized by the size of each component. Delay is measured in normalized gate delays, where 1$\tau$ is the delay of a single inverter driving four identical inverters. To simplify things, assume that the delay of a component is always the same regardless of the order in which different inputs arrive at a component. More specifically, the delay of a write access is the same regardless of whether the address arrives before the write data or vice versa. Also assume that we are using combinational memories (i.e., the address is set and the data is returned on the same cycle). 

\benu

% ======= Question 2.A ======= %
\item \textbf{Question 2.A -  Sequential Tag Check then Memory Access} \\

The diagram on the previous page illustrates two cache microarchitectures that serialize the tag check before data access. This means that for both reads and writes, the cache completely finishes the tag check and accesses the data memory only on a cache hit. figure (a) is for a directed-mapped cache, while figure (b) is for a two-way set-associative cache. We now want to determine the critical path and cycle time in units of $\tau$ for each cache microarchitecture. As an example, the table below shows the critical path and cycle time for the directed-mapped cache from (a). Note that because we are serializing the tag check before data access and because the delay equations are the same for both read and write accesses, the critical path is the same regardless of whether we are doing a read or a write. This is a simplification, but it will do for the purposes of this part. Note that the tag is 25 bits, but each row of the tag memory requires 26 bits since it must also include a valid bit. Create a table similar to the one in the example which identifies the critical path and cycle time in units of $\tau$ for the two-way set-associative cache in (b). Compare the cycle times of the two cache microarchitectures. What is the primary reason one microarchitecture is slower than the other microarchitecture?

\begin{table}[H]
\caption{Q2A Calculations} \label{tab:q2Atable} 
\begin{center}
\begin{tabular}{| l | l | l |}
\hline
  Component         &  Delay Eq.              & Delay($\tau$)          \\ \hline
  \V.addr_reg_MO.   &   \T{2}                 & \T{2}                  \\ \hline
  \V.tag_decoder.   &   \T{$2+2*2$}           & \T{6}                  \\ \hline
  \V.tag_mem.       &   \T{$12+(4+26)/32$}    & \T{13}                 \\ \hline
  \V.tag_cmp.       &   \T{$2+3$log$_2(26)$}  & \T{17}                 \\ \hline
  \V.tag_and.       &   \T{2}                 & \T{2}                  \\ \hline
  \V.tag_or.        &   \T{2}                 & \T{2}                  \\ \hline
  \V.data_mem.      &   \T{$12+(8+128)/32$}   & \T{17}                 \\ \hline
  \V.data_mux.      &   \T{$1+2$log$_2(4)$}   & \T{5}                  \\ \hline
  \V.rdata_reg_M1.  &   \T{1}                 & \T{1}                  \\ \hline \hline
  \V.Total.         &   \T{}                  & \T{65}                 \\ \hline
\end{tabular}
\end{center}
\end{table}


\textbf{Compare the Cycle times between the two cache microarchitectures, why is one slower than the other.} \\
\answer Well, one is slower than the other, but by my calculation this is only by a single cycle. The two-way set associative is one cycle faster, (clocking in at $65\tau$ vs $66\tau$ for the direct-mapped), and this is mostly due to the fact that it can decode half the tags in parallel, saving it 2 cycles compared to the direct-mapped, which must compare all 8 indexes with the same comparator. Accessing the tag memory saves another cycle, since we are once again searching through two halves in parallel. However, 2 of these saved cycles are un-done by the added OR-gate that compares the hit bits from both tag memories. I have a feeling that I made a mistake somewhere and the difference was supposed to be more pronounced, but in general an $n$-way associative cache will have a faster look-up time than an $m$-way ($m < n$) associative cache because of parallelization. This will come at the expense of more complicated hardware though. \\


% ======= Question 2.B ======= %
\item \textbf{Question 2.B - Parallel Read Hit Path} \\ 

The diagram on the next page illustrates two cache microarchitectures with parallel read hit paths and pipelined write hit paths. This means that for a single read request, the tag check is done in parallel with the data memory read access, while for for a single write request the tag check is done in stage M0 and the data memory write access is done in stage M1. Figure (a) is for a directed-mapped cache, while figure (b) is for a two-way set-associative cache. For this part we will focus just on the parallel read hit path for both the direct-mapped and set-associative caches. Create two tables similar to the one from the previous part which identifies the critical path and cycle time in units of τ for just the parallel read hit paths. Note that since the tag check and the data memory read access are done in parallel, you will need to examine both of these paths to determine which one is in fact the critical path. Compare the cycle times of the two cache microarchitectures. What is the primary reason one microarchitecture is slower than the other microarchitecture? \\

\answer The following is for the direct-mapped cache with parallel read and pipelined writes. To find the critical path, I actually had to calculate the times for 2 different paths, then compare those to find the longest. The first path I look at was the one from the \T{idx} line, through the \V.add_reg_M1., through the \V.idk mux., through the \V.data_decoder., and finally through the \V.data_memory. and \V.rdata_reg_M1.. This is the path that the circuit uses to forward the index to the data memory before the tag has even been compared for correctness. I found that this path takes $39\tau$ to finish. The second path I checked was the path that compared the tags, this path starts like the one in 2.A but doesn't end up going into the data memory (on reads). This path was the critical path and is listed below.

\begin{table}[H]
\caption{Q1D Calculations} \label{tab:q2Btable1} 
\begin{center}
\begin{tabular}{| l | l | l |}
\hline
  Component         &  Delay Eq.              & Delay($\tau$)          \\ \hline
  \V.addr_reg_MO.   &   \T{2}                 & \T{2}                  \\ \hline
  \V.tag_decoder.   &   \T{$2+2*3$}           & \T{8}                  \\ \hline
  \V.tag_mem.       &   \T{$12+(8+26)/32$}    & \T{14}                 \\ \hline
  \V.tag_cmp.       &   \T{$2+3$log$_2(26)$}  & \T{17}                 \\ \hline
  \V.tag_and.       &   \T{2}                 & \T{2}                  \\ \hline
  \V.hit_reg_M1.    &   \T{1}                 & \T{1}                  \\ \hline
  \V.Total.         &   \T{}                  & \T{44}                 \\ \hline
\end{tabular}
\end{center}
\end{table}

The above table shows that by using a parallel structure to grab the data while also verifying it is valid reduces the time needed to grab a set of data from the cache from $66\tau$ to $44\tau$, a significant improvement, at the cost of increased power draw (we read from the data memory even when we don't need to).

  
% ======= Question 2.C ======= %
\item \textbf{Question 2.C - Pipelined Write Hit Path} \\
For this part we will focus just on the pipelined write hit path for both the direct-mapped and set-associative caches shown on the next page. Create two tables similar to the one in the previous parts which identifies the critical path and cycle time in units of τ for just the pipelined write hit path. Note that since the tag check and the data memory write access happen in two different stages, you will need to examine both of these paths to determine which one is in fact the critical path. Compare the cycle times of the two cache microarchitectures. What is the primary reason one microarchitecture is slower than the other microarchitecture?


\enu % end problem #2 sub-problems



%==============================%
% ======== PROBLEM #3 ======== %
%==============================%
\item  \textbf{Question \#3 : Cache Hierarchies} \\
Assume a processor uses a dedicated L1 cache for instructions (IL1) and a dedicated L1 cache for data (DL1). The processor also uses a shared L2 cache that serves as an intermediate level between each of the L1 caches and memory.  
  
The processor has a cycle time of 1 ns (i.e., operates at 1 GHz frequency). A hit to either IL1 or DL1 takes 1 cycle (time to look up the cache and return the data to processor). IL1 has 8\% miss rate. DL1 has 15\% miss rate. Assume load/store instructions comprise 25\% of the total instructions. Hit access to L2 from either of the IL1 or DL1 caches takes 6 cycles (time to lookup the cache and return the data to either of higher level caches). L2 has total miss rate of 30\%. From L2, it takes 50 cycles to access memory
\benu

% ======= Question 3.A ======= %
\item \textbf{Question 3.A AMAT} \\
What is the average memory access time? \\

\answer

% ======= Question 3.B ======= %
\item \textbf{Question 3.B AMAT} \\
What is the average memory access time? \\

\answer

\enu



%==============================%
% ======== PROBLEM #4 ======== %
%==============================%
\item \textbf{Question \#4 : } \\


\benu 

\item nothing
\enu




%==============================%
% ======== PROBLEM #5 ======== %
%==============================%
\item \textbf{Question \#5 : Multipliers}
  
  \benu
\item nothing
  \enu



%==============================%
% ======== PROBLEM #6 ======== %
%==============================%
\item \textbf{Question \#6 : Hazards} \\
 
\benu

\item nothing
\enu
\enu
\end{document}
